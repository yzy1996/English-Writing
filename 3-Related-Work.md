# Related Works

> 这一部分是由很多小节构成，小节之间可以没有连贯性。在这里举例一个小节的多种写作方式，可以自由组合调整



| 先笼统介绍，再细说一些方法的核心区别                         |
| :----------------------------------------------------------- |
| **Some using full-supervision** in the form of semantic labels, **others** find meaningful directions **in a self-supervised fashion**, and, **finally**, recent works present **unsupervised methods** to achieve the same goal. |
| **More specifically**, **xx use** supervision in the form of facial attribute labels to find meaningful linear direction... **yy perform** eigenvector decomposition on the generator's weights to find edit directions without additional supervision. |



| 一句话大概说说这个领域有很多方法                             |
| :----------------------------------------------------------- |
| The most related to our work can be divided into two categories: A and B. The first focuses on xx ; The second xx |



| 针对每一个方法，先概述数量多少                            |
| :-------------------------------------------------------- |
| Only very few works have explored xx in the context of xx |



| 有很多方法为了解决什么问题，但他们都只关心了。。             |
| :----------------------------------------------------------- |
| We review recent advances in xx, as well as xx.              |
| xx are related to many other bodies of work.                 |
| Our method build upon this line of research                  |
| Its formulation has been adapted for improving A, dealing with B, removing C, and handling D. Although there have been some early attempts in E, the usage of F has never been explored for G. This work addresses this gap by proposing H as I that leverages K. |
| Towards this goal                                            |





## 同期工作

| 最后做个比较                                                 |
| ------------------------------------------------------------ |
| The concurrently developed 3D-aware GANs StyleNeRF and CIPS-3D [73] demonstrate impressive image quality. The central distinction between these and ours is that while StyleNeRF and CIPS-3D operate primarily in image-space, with less emphasis on the 3D representation, our method operates primarily in 3D. Our approach demonstrates greater view consistency, and is capable of generating high-quality 3D shapes. |
|                                                              |
|                                                              |

